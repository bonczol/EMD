{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "c5QKGfZDSvtZ"
      },
      "source": [
        "# word2vec\n",
        "\n",
        "Ten notatnik ma na celu przedstawienie sposobu tworzenia i wykorzystania reprezentacji werktorowych na przykładzie algorytmu word2vec. W trakcie zadania najpierw stworzymy prostą reprezentację wektorową, a następnie spróbujemy wczytać gotowy model nauczony na dużym korpusie tekstowym.\n",
        "\n",
        "Po wykonaniu tego zadania powinieneś:\n",
        "+ wiedzieć na czym polega word2vec,\n",
        "+ potrafić stworzyć word2vec na własnych danych,\n",
        "+ potrafić wykorzystać word2vec do:\n",
        "\t+ znalezienia podobnych słów,\n",
        "\t+ wyszukiwania słów na zasadzie \"reguły trzech\", \n",
        "\t+ wykrywania niepasujących słów,\n",
        "\t+ do tworzenia wektora cech nadającego się do klasyfikacji,\n",
        "+ wczytać i wykorzystać gotowy model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNnCvw_bSvth"
      },
      "source": [
        "## Prosty model\n",
        "\n",
        "Najpierw wczytamy odpowiednie biblioteki i stworzymy mały zbiór treningowy na podstawie znanej piosenki."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETwkJz1jSvti"
      },
      "source": [
        "import gensim, logging, re, nltk\n",
        "import pandas as pd\n",
        "\n",
        "RE_SPACES = re.compile(\"\\s+\")\n",
        "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
        "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
        "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        " \n",
        "song = \"\"\"Gdzie strumyk płynie z wolna,\n",
        "Rozsiewa zioła maj,\n",
        "Stokrotka rosła polna,\n",
        "A nad nią szumiał gaj,\n",
        "Stokrotka rosła polna,\n",
        "A nad nią szumiał gaj,\n",
        "Zielony gaj.\n",
        "\n",
        "W tym gaju tak ponuro,\n",
        "Że aż przeraża mnie,\n",
        "Ptaszęta za wysoko,\n",
        "A mnie samotnej źle,\n",
        "Ptaszęta za wysoko,\n",
        "A mnie samotnej źle,\n",
        "samotnej źle.\n",
        "\n",
        "Wtem harcerz idzie z wolna.\n",
        "„Stokrotko, witam cię,\n",
        "Twój urok mnie zachwyca,\n",
        "Czy chcesz być mą, czy nie?”\n",
        "\"Twój urok mnie zachwyca,\n",
        "Czy chcesz być mą, czy nie?\n",
        "Czy nie, czy nie?\n",
        "\n",
        "Stokrotka się zgodziła\n",
        "I poszli w ciemny las,\n",
        "A harcerz taki gapa\n",
        "że aż w pokrzywy wlazł,\n",
        "A harcerz taki gapa\n",
        "że aż w pokrzywy wlazł,\n",
        "w pokrzywy wlazł.\n",
        "\n",
        "A ona, ona, ona,\n",
        "Cóż biedna robić ma,\n",
        "Nad gapą pochylona\n",
        "I śmieje się: ha, ha,\n",
        "Nad gapą pochylona\n",
        "I śmieje: się ha, ha,\n",
        "ha, ha, ha, ha.\"\"\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-t0atK-Svtj"
      },
      "source": [
        "**Zad. 1: Podziel piosenkę na wersy, a wersy tokenizuj spacjami. W efekcie powinieneś stworzyć listę list i przypisać ją do zmiennej `sentences`.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQBu_4MFSvtj",
        "outputId": "c7d77011-6641-405b-93a2-cae4938ed648"
      },
      "source": [
        "sentences =  []\n",
        "for i in song.splitlines():\n",
        "  a=[]\n",
        "  for j in i.split():\n",
        "    a.append(j)\n",
        "  sentences.append(a)\n",
        "\n",
        "print(sentences)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Gdzie', 'strumyk', 'płynie', 'z', 'wolna,'], ['Rozsiewa', 'zioła', 'maj,'], ['Stokrotka', 'rosła', 'polna,'], ['A', 'nad', 'nią', 'szumiał', 'gaj,'], ['Stokrotka', 'rosła', 'polna,'], ['A', 'nad', 'nią', 'szumiał', 'gaj,'], ['Zielony', 'gaj.'], [], ['W', 'tym', 'gaju', 'tak', 'ponuro,'], ['Że', 'aż', 'przeraża', 'mnie,'], ['Ptaszęta', 'za', 'wysoko,'], ['A', 'mnie', 'samotnej', 'źle,'], ['Ptaszęta', 'za', 'wysoko,'], ['A', 'mnie', 'samotnej', 'źle,'], ['samotnej', 'źle.'], [], ['Wtem', 'harcerz', 'idzie', 'z', 'wolna.'], ['„Stokrotko,', 'witam', 'cię,'], ['Twój', 'urok', 'mnie', 'zachwyca,'], ['Czy', 'chcesz', 'być', 'mą,', 'czy', 'nie?”'], ['\"Twój', 'urok', 'mnie', 'zachwyca,'], ['Czy', 'chcesz', 'być', 'mą,', 'czy', 'nie?'], ['Czy', 'nie,', 'czy', 'nie?'], [], ['Stokrotka', 'się', 'zgodziła'], ['I', 'poszli', 'w', 'ciemny', 'las,'], ['A', 'harcerz', 'taki', 'gapa'], ['że', 'aż', 'w', 'pokrzywy', 'wlazł,'], ['A', 'harcerz', 'taki', 'gapa'], ['że', 'aż', 'w', 'pokrzywy', 'wlazł,'], ['w', 'pokrzywy', 'wlazł.'], [], ['A', 'ona,', 'ona,', 'ona,'], ['Cóż', 'biedna', 'robić', 'ma,'], ['Nad', 'gapą', 'pochylona'], ['I', 'śmieje', 'się:', 'ha,', 'ha,'], ['Nad', 'gapą', 'pochylona'], ['I', 'śmieje:', 'się', 'ha,', 'ha,'], ['ha,', 'ha,', 'ha,', 'ha.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVhOEtAgSvtk"
      },
      "source": [
        "Mając tekst podzielony na zdania a zdania na tokeny, możemy nauczyć model word2vec.\n",
        "\n",
        "**Zad. 2: Naucz model word2vec. [Sprawdź](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) za co odpowiedzialne są parametry `min_count` i `iter`. Jakie inne parametry mogą być przydatne?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "id": "h-p46qeZSvtl",
        "outputId": "90d4a911-a0b7-46c9-ac1a-179f93cde748"
      },
      "source": [
        "model = gensim.models.Word2Vec(sentences, iter=5, min_count=1)\n",
        "print(model)\n",
        "print(model.vocabulary)\n",
        "\n",
        "model.wv.doesnt_match(\"las harcerz gaj zioła\".split())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-19 17:32:54,091 : INFO : collecting all words and their counts\n",
            "2021-01-19 17:32:54,092 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2021-01-19 17:32:54,094 : INFO : collected 79 word types from a corpus of 140 raw words and 39 sentences\n",
            "2021-01-19 17:32:54,096 : INFO : Loading a fresh vocabulary\n",
            "2021-01-19 17:32:54,098 : INFO : effective_min_count=1 retains 79 unique words (100% of original 79, drops 0)\n",
            "2021-01-19 17:32:54,099 : INFO : effective_min_count=1 leaves 140 word corpus (100% of original 140, drops 0)\n",
            "2021-01-19 17:32:54,102 : INFO : deleting the raw counts dictionary of 79 items\n",
            "2021-01-19 17:32:54,103 : INFO : sample=0.001 downsamples 79 most-common words\n",
            "2021-01-19 17:32:54,104 : INFO : downsampling leaves estimated 48 word corpus (35.0% of prior 140)\n",
            "2021-01-19 17:32:54,105 : INFO : estimated required memory for 79 words and 100 dimensions: 102700 bytes\n",
            "2021-01-19 17:32:54,106 : INFO : resetting layer weights\n",
            "2021-01-19 17:32:54,150 : INFO : training model with 3 workers on 79 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2021-01-19 17:32:54,155 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:32:54,156 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:32:54,156 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:32:54,159 : INFO : EPOCH - 1 : training on 140 raw words (53 effective words) took 0.0s, 10955 effective words/s\n",
            "2021-01-19 17:32:54,167 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:32:54,168 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:32:54,171 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:32:54,173 : INFO : EPOCH - 2 : training on 140 raw words (46 effective words) took 0.0s, 6750 effective words/s\n",
            "2021-01-19 17:32:54,177 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:32:54,177 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:32:54,179 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:32:54,179 : INFO : EPOCH - 3 : training on 140 raw words (46 effective words) took 0.0s, 13023 effective words/s\n",
            "2021-01-19 17:32:54,191 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:32:54,193 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:32:54,197 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:32:54,198 : INFO : EPOCH - 4 : training on 140 raw words (54 effective words) took 0.0s, 6874 effective words/s\n",
            "2021-01-19 17:32:54,205 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:32:54,206 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:32:54,207 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:32:54,207 : INFO : EPOCH - 5 : training on 140 raw words (51 effective words) took 0.0s, 23567 effective words/s\n",
            "2021-01-19 17:32:54,207 : INFO : training on a 700 raw words (250 effective words) took 0.1s, 4370 effective words/s\n",
            "2021-01-19 17:32:54,208 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "2021-01-19 17:32:54,209 : INFO : precomputing L2-norms of word weight vectors\n",
            "2021-01-19 17:32:54,209 : WARNING : vectors for words {'las', 'gaj'} are not present in the model, ignoring these words\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=79, size=100, alpha=0.025)\n",
            "<gensim.models.word2vec.Word2VecVocab object at 0x7fbf741cf4e0>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'zioła'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioccsX-4Svtl"
      },
      "source": [
        "Model jest niezwykle mały i niezbyt praktyczny, ale pozwolił pokazać podstawę uczenia word2vec. Przy większych korpusach tekstowych wczytywanie do pamięci wielkich tablic nie byłoby najlepszym pomysłem. Na szczęście implementacja word2vec w gensim potrafi przetwarzać dane przyrostowo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaW9gPgjSvtm"
      },
      "source": [
        "## Przetwarzanie strumieniowe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPbSwRO6Svtm"
      },
      "source": [
        "Zamiast wczytywać wszystkie dokumenty naraz można robić to partiami, bo sieci neuronowe (w tym word2vec) potrafią douczać się przyrostowo. Do douczania przyrostowego świetnie nada się pythonowy iterator lub generator. Jeśli nie kojarzysz na czym polega działanie iteratorów i generatorów, zobacz jak [wyjaśnia to Radim Rehurek](https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/).\n",
        "\n",
        "Zasymulujmy zdania/wersy/tweety przechowywane w osobnych plikach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ly1rcyJSvtm"
      },
      "source": [
        "import smart_open, os\n",
        "\n",
        "if not os.path.exists('./data/'):\n",
        "    os.makedirs('./data/')\n",
        "\n",
        "filenames = ['./data/f' + str(i) +'.txt' for i in range(39)]\n",
        "\n",
        "if sentences is not None:\n",
        "    for i, fname in enumerate(filenames):\n",
        "        with smart_open.smart_open(fname, 'w') as fout:\n",
        "            for line in sentences[i]:\n",
        "                fout.write(line + ' ')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P5QxkUDSvtn"
      },
      "source": [
        "**Zad. 3: Mając powyższy zbiór dokumentów tekstowych, stwórz metodę która będzie \"leniwie\" iterowała przez zasymulowany zbiór danych. Podczas iterowania usuń znaki interpunkcyjne i zmień wszystkie litery na małe.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HsmwipWcSvtn",
        "outputId": "fe9b9677-46fe-406f-f92f-dcc95e84d178"
      },
      "source": [
        "import string\n",
        "\n",
        "def clean(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "class MySentences(object):\n",
        "    def __init__(self, dirname):\n",
        "        self.dirname = dirname\n",
        " \n",
        "    def __iter__(self):\n",
        "        for fname in os.listdir(self.dirname):\n",
        "            if fname.endswith('.txt'):\n",
        "                for line in open(os.path.join(self.dirname, fname)):\n",
        "                  if len(line)>0:\n",
        "                    yield [clean(x) for x in line.split()]\n",
        "\n",
        "# Do odkomentowania:\n",
        "sentences = MySentences('./data/')\n",
        "model = gensim.models.Word2Vec(sentences, iter=10, min_count=1)\n",
        "print(model)\n",
        "\n",
        "model.wv.doesnt_match(\"las harcerz gaj zioła\".split())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-19 17:34:57,375 : INFO : collecting all words and their counts\n",
            "2021-01-19 17:34:57,376 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2021-01-19 17:34:57,387 : INFO : collected 65 word types from a corpus of 140 raw words and 35 sentences\n",
            "2021-01-19 17:34:57,388 : INFO : Loading a fresh vocabulary\n",
            "2021-01-19 17:34:57,390 : INFO : effective_min_count=1 retains 65 unique words (100% of original 65, drops 0)\n",
            "2021-01-19 17:34:57,391 : INFO : effective_min_count=1 leaves 140 word corpus (100% of original 140, drops 0)\n",
            "2021-01-19 17:34:57,393 : INFO : deleting the raw counts dictionary of 65 items\n",
            "2021-01-19 17:34:57,394 : INFO : sample=0.001 downsamples 65 most-common words\n",
            "2021-01-19 17:34:57,395 : INFO : downsampling leaves estimated 43 word corpus (30.9% of prior 140)\n",
            "2021-01-19 17:34:57,397 : INFO : estimated required memory for 65 words and 100 dimensions: 84500 bytes\n",
            "2021-01-19 17:34:57,398 : INFO : resetting layer weights\n",
            "2021-01-19 17:34:57,423 : INFO : training model with 3 workers on 65 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2021-01-19 17:34:57,431 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:34:57,431 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:34:57,433 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:34:57,434 : INFO : EPOCH - 1 : training on 140 raw words (46 effective words) took 0.0s, 6714 effective words/s\n",
            "2021-01-19 17:34:57,444 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:34:57,445 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:34:57,446 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:34:57,446 : INFO : EPOCH - 2 : training on 140 raw words (41 effective words) took 0.0s, 4161 effective words/s\n",
            "2021-01-19 17:34:57,459 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:34:57,459 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:34:57,461 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:34:57,462 : INFO : EPOCH - 3 : training on 140 raw words (37 effective words) took 0.0s, 3039 effective words/s\n",
            "2021-01-19 17:34:57,472 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:34:57,472 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:34:57,473 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:34:57,475 : INFO : EPOCH - 4 : training on 140 raw words (46 effective words) took 0.0s, 7080 effective words/s\n",
            "2021-01-19 17:34:57,485 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:34:57,486 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:34:57,487 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:34:57,488 : INFO : EPOCH - 5 : training on 140 raw words (42 effective words) took 0.0s, 6766 effective words/s\n",
            "2021-01-19 17:34:57,499 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:34:57,500 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:34:57,501 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:34:57,502 : INFO : EPOCH - 6 : training on 140 raw words (44 effective words) took 0.0s, 6033 effective words/s\n",
            "2021-01-19 17:34:57,512 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:34:57,512 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:34:57,515 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:34:57,518 : INFO : EPOCH - 7 : training on 140 raw words (45 effective words) took 0.0s, 4657 effective words/s\n",
            "2021-01-19 17:34:57,530 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:34:57,531 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:34:57,532 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:34:57,533 : INFO : EPOCH - 8 : training on 140 raw words (49 effective words) took 0.0s, 5581 effective words/s\n",
            "2021-01-19 17:34:57,545 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:34:57,547 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:34:57,548 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:34:57,549 : INFO : EPOCH - 9 : training on 140 raw words (41 effective words) took 0.0s, 3154 effective words/s\n",
            "2021-01-19 17:34:57,559 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:34:57,560 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:34:57,561 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:34:57,561 : INFO : EPOCH - 10 : training on 140 raw words (43 effective words) took 0.0s, 7388 effective words/s\n",
            "2021-01-19 17:34:57,565 : INFO : training on a 1400 raw words (434 effective words) took 0.1s, 3080 effective words/s\n",
            "2021-01-19 17:34:57,565 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "2021-01-19 17:34:57,566 : INFO : precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=65, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'harcerz'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FERQ5lXbSvtn"
      },
      "source": [
        "## Trochę więcej danych i przykłady zastosowań\n",
        "\n",
        "Jak wspomniano wcześniej powyższa piosenka jest zbyt krótka by stworzyć przekonujący model podobieństwa między słowami. Przejdziemy teraz na język angielski i wykorzystamy korpus dołączony do biblioteki `gensim`. Ten korpus nie jest jeszcze duży, więc wyniki nie będą rewelacyjne. Potrzeba > 500 tys. słów, żeby oczekiwać rozsądnych wyników dla ogólnych zapytań, ale przy specjalistycznych zastosowaniach korpusy niekoniecznie muszą być takie duże.\n",
        "\n",
        "**Zad. 4: Korzystając ze zdobytej wiedzy na temat iteratorów, uzupełnij poniższy kod.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD9UFqG4Svtn"
      },
      "source": [
        "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n",
        "lee_train_file = test_data_dir + 'lee_background.cor'\n",
        "\n",
        "class MyText(object):\n",
        "    def __iter__(self):\n",
        "        for line in open(lee_train_file):\n",
        "            # Załóż, że każda linia to dokument, zmień litery na małe,\n",
        "            # usuń podstawowe znaki interpunkcyjne i podziel według białych znaków\n",
        "\n",
        "            for l in line.splitlines():\n",
        "              if len(l)>0:\n",
        "                yield [clean(x) for x in l.split()]\n",
        "            \n",
        "\n",
        "sentences = MyText()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_wnsZWVSvto"
      },
      "source": [
        "**Zad. 5: Naucz model word2vec o rozmiarze 200, przez 100 epok, usuwając słowa występującerzadziej niż 5 razy. Wynik przypisz do zmiennej `model`.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCyS0dH_Svto",
        "outputId": "18cf64e3-c5c8-4fab-fa41-6c8359ba9019"
      },
      "source": [
        "model = gensim.models.Word2Vec(sentences, size=200, iter=100, min_count=5)\n",
        "print(model)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-19 17:46:07,360 : INFO : collecting all words and their counts\n",
            "2021-01-19 17:46:07,367 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2021-01-19 17:46:07,899 : INFO : collected 7279 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2021-01-19 17:46:07,900 : INFO : Loading a fresh vocabulary\n",
            "2021-01-19 17:46:07,910 : INFO : effective_min_count=5 retains 1744 unique words (23% of original 7279, drops 5535)\n",
            "2021-01-19 17:46:07,910 : INFO : effective_min_count=5 leaves 50671 word corpus (84% of original 59890, drops 9219)\n",
            "2021-01-19 17:46:07,919 : INFO : deleting the raw counts dictionary of 7279 items\n",
            "2021-01-19 17:46:07,920 : INFO : sample=0.001 downsamples 51 most-common words\n",
            "2021-01-19 17:46:07,921 : INFO : downsampling leaves estimated 35977 word corpus (71.0% of prior 50671)\n",
            "2021-01-19 17:46:07,928 : INFO : estimated required memory for 1744 words and 200 dimensions: 3662400 bytes\n",
            "2021-01-19 17:46:07,930 : INFO : resetting layer weights\n",
            "2021-01-19 17:46:08,300 : INFO : training model with 3 workers on 1744 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2021-01-19 17:46:08,908 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:08,912 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:08,925 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:08,926 : INFO : EPOCH - 1 : training on 59890 raw words (36000 effective words) took 0.6s, 58026 effective words/s\n",
            "2021-01-19 17:46:09,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:09,525 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:09,535 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:09,536 : INFO : EPOCH - 2 : training on 59890 raw words (36063 effective words) took 0.6s, 59344 effective words/s\n",
            "2021-01-19 17:46:10,133 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:10,136 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:10,148 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:10,149 : INFO : EPOCH - 3 : training on 59890 raw words (35898 effective words) took 0.6s, 58846 effective words/s\n",
            "2021-01-19 17:46:10,740 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:10,745 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:10,753 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:10,754 : INFO : EPOCH - 4 : training on 59890 raw words (35950 effective words) took 0.6s, 59649 effective words/s\n",
            "2021-01-19 17:46:11,339 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:11,340 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:11,350 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:11,350 : INFO : EPOCH - 5 : training on 59890 raw words (35973 effective words) took 0.6s, 60581 effective words/s\n",
            "2021-01-19 17:46:11,950 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:11,955 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:11,965 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:11,965 : INFO : EPOCH - 6 : training on 59890 raw words (36030 effective words) took 0.6s, 58878 effective words/s\n",
            "2021-01-19 17:46:12,557 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:12,562 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:12,573 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:12,573 : INFO : EPOCH - 7 : training on 59890 raw words (35922 effective words) took 0.6s, 59545 effective words/s\n",
            "2021-01-19 17:46:13,164 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:13,171 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:13,178 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:13,179 : INFO : EPOCH - 8 : training on 59890 raw words (35987 effective words) took 0.6s, 60013 effective words/s\n",
            "2021-01-19 17:46:13,775 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:13,779 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:13,791 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:13,792 : INFO : EPOCH - 9 : training on 59890 raw words (35895 effective words) took 0.6s, 59089 effective words/s\n",
            "2021-01-19 17:46:14,377 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:14,379 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:14,392 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:14,393 : INFO : EPOCH - 10 : training on 59890 raw words (35902 effective words) took 0.6s, 59956 effective words/s\n",
            "2021-01-19 17:46:14,986 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:14,990 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:15,001 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:15,002 : INFO : EPOCH - 11 : training on 59890 raw words (35977 effective words) took 0.6s, 59317 effective words/s\n",
            "2021-01-19 17:46:15,603 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:15,607 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:15,620 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:15,621 : INFO : EPOCH - 12 : training on 59890 raw words (36038 effective words) took 0.6s, 58821 effective words/s\n",
            "2021-01-19 17:46:16,213 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:16,215 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:16,229 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:16,230 : INFO : EPOCH - 13 : training on 59890 raw words (35984 effective words) took 0.6s, 59542 effective words/s\n",
            "2021-01-19 17:46:16,828 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:16,829 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:16,844 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:16,845 : INFO : EPOCH - 14 : training on 59890 raw words (35992 effective words) took 0.6s, 58730 effective words/s\n",
            "2021-01-19 17:46:17,450 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:17,451 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:17,468 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:17,468 : INFO : EPOCH - 15 : training on 59890 raw words (35904 effective words) took 0.6s, 57802 effective words/s\n",
            "2021-01-19 17:46:18,066 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:18,067 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:18,080 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:18,080 : INFO : EPOCH - 16 : training on 59890 raw words (35917 effective words) took 0.6s, 59654 effective words/s\n",
            "2021-01-19 17:46:18,682 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:18,684 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:18,695 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:18,696 : INFO : EPOCH - 17 : training on 59890 raw words (35847 effective words) took 0.6s, 58944 effective words/s\n",
            "2021-01-19 17:46:19,284 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:19,286 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:19,299 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:19,300 : INFO : EPOCH - 18 : training on 59890 raw words (36011 effective words) took 0.6s, 60160 effective words/s\n",
            "2021-01-19 17:46:19,894 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:19,902 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:19,910 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:19,911 : INFO : EPOCH - 19 : training on 59890 raw words (35872 effective words) took 0.6s, 58944 effective words/s\n",
            "2021-01-19 17:46:20,504 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:20,508 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:20,526 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:20,527 : INFO : EPOCH - 20 : training on 59890 raw words (35956 effective words) took 0.6s, 58621 effective words/s\n",
            "2021-01-19 17:46:21,123 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:21,128 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:21,140 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:21,141 : INFO : EPOCH - 21 : training on 59890 raw words (36034 effective words) took 0.6s, 59504 effective words/s\n",
            "2021-01-19 17:46:21,736 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:21,737 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:21,751 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:21,752 : INFO : EPOCH - 22 : training on 59890 raw words (35931 effective words) took 0.6s, 59013 effective words/s\n",
            "2021-01-19 17:46:22,349 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:22,353 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:22,366 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:22,366 : INFO : EPOCH - 23 : training on 59890 raw words (35994 effective words) took 0.6s, 59248 effective words/s\n",
            "2021-01-19 17:46:22,966 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:22,970 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:22,983 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:22,984 : INFO : EPOCH - 24 : training on 59890 raw words (36006 effective words) took 0.6s, 58632 effective words/s\n",
            "2021-01-19 17:46:23,585 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:23,587 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:23,600 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:23,601 : INFO : EPOCH - 25 : training on 59890 raw words (36012 effective words) took 0.6s, 58835 effective words/s\n",
            "2021-01-19 17:46:24,187 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:24,189 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:24,202 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:24,203 : INFO : EPOCH - 26 : training on 59890 raw words (35950 effective words) took 0.6s, 59975 effective words/s\n",
            "2021-01-19 17:46:24,822 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:24,826 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:24,838 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:24,839 : INFO : EPOCH - 27 : training on 59890 raw words (36070 effective words) took 0.6s, 57050 effective words/s\n",
            "2021-01-19 17:46:25,430 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:25,431 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:25,445 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:25,446 : INFO : EPOCH - 28 : training on 59890 raw words (35945 effective words) took 0.6s, 59682 effective words/s\n",
            "2021-01-19 17:46:26,049 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:26,054 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:26,065 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:26,066 : INFO : EPOCH - 29 : training on 59890 raw words (36039 effective words) took 0.6s, 58410 effective words/s\n",
            "2021-01-19 17:46:26,672 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:26,673 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:26,688 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:26,688 : INFO : EPOCH - 30 : training on 59890 raw words (35996 effective words) took 0.6s, 58131 effective words/s\n",
            "2021-01-19 17:46:27,276 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:27,277 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:27,294 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:27,295 : INFO : EPOCH - 31 : training on 59890 raw words (35986 effective words) took 0.6s, 59551 effective words/s\n",
            "2021-01-19 17:46:27,892 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:27,896 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:27,909 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:27,910 : INFO : EPOCH - 32 : training on 59890 raw words (35912 effective words) took 0.6s, 58989 effective words/s\n",
            "2021-01-19 17:46:28,494 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:28,496 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:28,512 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:28,513 : INFO : EPOCH - 33 : training on 59890 raw words (35989 effective words) took 0.6s, 60009 effective words/s\n",
            "2021-01-19 17:46:29,115 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:29,117 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:29,132 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:29,132 : INFO : EPOCH - 34 : training on 59890 raw words (35934 effective words) took 0.6s, 58571 effective words/s\n",
            "2021-01-19 17:46:29,734 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:29,735 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:29,749 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:29,750 : INFO : EPOCH - 35 : training on 59890 raw words (35896 effective words) took 0.6s, 58553 effective words/s\n",
            "2021-01-19 17:46:30,340 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:30,342 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:30,356 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:30,356 : INFO : EPOCH - 36 : training on 59890 raw words (35984 effective words) took 0.6s, 59886 effective words/s\n",
            "2021-01-19 17:46:30,948 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:30,950 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:30,965 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:30,966 : INFO : EPOCH - 37 : training on 59890 raw words (35905 effective words) took 0.6s, 59822 effective words/s\n",
            "2021-01-19 17:46:31,565 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:31,567 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:31,580 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:31,581 : INFO : EPOCH - 38 : training on 59890 raw words (36099 effective words) took 0.6s, 58938 effective words/s\n",
            "2021-01-19 17:46:32,184 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:32,186 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:32,200 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:32,201 : INFO : EPOCH - 39 : training on 59890 raw words (35950 effective words) took 0.6s, 58757 effective words/s\n",
            "2021-01-19 17:46:32,795 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:32,797 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:32,810 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:32,811 : INFO : EPOCH - 40 : training on 59890 raw words (35850 effective words) took 0.6s, 59036 effective words/s\n",
            "2021-01-19 17:46:33,399 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:33,400 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:33,413 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:33,414 : INFO : EPOCH - 41 : training on 59890 raw words (36004 effective words) took 0.6s, 59935 effective words/s\n",
            "2021-01-19 17:46:34,021 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:34,024 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:34,037 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:34,037 : INFO : EPOCH - 42 : training on 59890 raw words (35957 effective words) took 0.6s, 58290 effective words/s\n",
            "2021-01-19 17:46:34,622 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:34,628 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:34,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:34,639 : INFO : EPOCH - 43 : training on 59890 raw words (36089 effective words) took 0.6s, 60256 effective words/s\n",
            "2021-01-19 17:46:35,242 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:35,247 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:35,258 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:35,259 : INFO : EPOCH - 44 : training on 59890 raw words (35961 effective words) took 0.6s, 58294 effective words/s\n",
            "2021-01-19 17:46:35,852 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:35,854 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:35,868 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:35,869 : INFO : EPOCH - 45 : training on 59890 raw words (35879 effective words) took 0.6s, 59187 effective words/s\n",
            "2021-01-19 17:46:36,456 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:36,460 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:36,472 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:36,473 : INFO : EPOCH - 46 : training on 59890 raw words (35995 effective words) took 0.6s, 59813 effective words/s\n",
            "2021-01-19 17:46:37,075 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:37,077 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:37,091 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:37,092 : INFO : EPOCH - 47 : training on 59890 raw words (36038 effective words) took 0.6s, 58715 effective words/s\n",
            "2021-01-19 17:46:37,683 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:37,685 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:37,698 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:37,698 : INFO : EPOCH - 48 : training on 59890 raw words (35829 effective words) took 0.6s, 59646 effective words/s\n",
            "2021-01-19 17:46:38,290 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:38,292 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:38,308 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:38,309 : INFO : EPOCH - 49 : training on 59890 raw words (35943 effective words) took 0.6s, 59148 effective words/s\n",
            "2021-01-19 17:46:38,909 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:38,915 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:38,925 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:38,926 : INFO : EPOCH - 50 : training on 59890 raw words (36048 effective words) took 0.6s, 58929 effective words/s\n",
            "2021-01-19 17:46:39,512 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:39,518 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:39,527 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:39,528 : INFO : EPOCH - 51 : training on 59890 raw words (35986 effective words) took 0.6s, 60225 effective words/s\n",
            "2021-01-19 17:46:40,132 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:40,134 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:40,148 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:40,149 : INFO : EPOCH - 52 : training on 59890 raw words (36014 effective words) took 0.6s, 58761 effective words/s\n",
            "2021-01-19 17:46:40,737 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:40,738 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:40,753 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:40,754 : INFO : EPOCH - 53 : training on 59890 raw words (36002 effective words) took 0.6s, 60635 effective words/s\n",
            "2021-01-19 17:46:41,360 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:41,361 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:41,376 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:41,376 : INFO : EPOCH - 54 : training on 59890 raw words (36007 effective words) took 0.6s, 58140 effective words/s\n",
            "2021-01-19 17:46:41,978 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:41,983 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:41,993 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:41,994 : INFO : EPOCH - 55 : training on 59890 raw words (35933 effective words) took 0.6s, 58416 effective words/s\n",
            "2021-01-19 17:46:42,589 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:42,591 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:42,604 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:42,605 : INFO : EPOCH - 56 : training on 59890 raw words (36137 effective words) took 0.6s, 59398 effective words/s\n",
            "2021-01-19 17:46:43,197 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:43,199 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:43,213 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:43,213 : INFO : EPOCH - 57 : training on 59890 raw words (35904 effective words) took 0.6s, 59262 effective words/s\n",
            "2021-01-19 17:46:43,802 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:43,806 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:43,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:43,822 : INFO : EPOCH - 58 : training on 59890 raw words (36005 effective words) took 0.6s, 59776 effective words/s\n",
            "2021-01-19 17:46:44,411 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:44,413 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:44,427 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:44,428 : INFO : EPOCH - 59 : training on 59890 raw words (35959 effective words) took 0.6s, 60299 effective words/s\n",
            "2021-01-19 17:46:45,032 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:45,037 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:45,048 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:45,048 : INFO : EPOCH - 60 : training on 59890 raw words (36010 effective words) took 0.6s, 58272 effective words/s\n",
            "2021-01-19 17:46:45,636 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:45,638 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:45,650 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:45,651 : INFO : EPOCH - 61 : training on 59890 raw words (36022 effective words) took 0.6s, 60331 effective words/s\n",
            "2021-01-19 17:46:46,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:46,245 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:46,259 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:46,260 : INFO : EPOCH - 62 : training on 59890 raw words (36038 effective words) took 0.6s, 59450 effective words/s\n",
            "2021-01-19 17:46:46,860 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:46,864 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:46,876 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:46,877 : INFO : EPOCH - 63 : training on 59890 raw words (35990 effective words) took 0.6s, 58567 effective words/s\n",
            "2021-01-19 17:46:47,472 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:47,474 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:47,487 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:47,488 : INFO : EPOCH - 64 : training on 59890 raw words (35978 effective words) took 0.6s, 59188 effective words/s\n",
            "2021-01-19 17:46:48,106 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:48,108 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:48,121 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:48,121 : INFO : EPOCH - 65 : training on 59890 raw words (35982 effective words) took 0.6s, 57257 effective words/s\n",
            "2021-01-19 17:46:48,713 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:48,714 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:48,730 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:48,730 : INFO : EPOCH - 66 : training on 59890 raw words (35875 effective words) took 0.6s, 59531 effective words/s\n",
            "2021-01-19 17:46:49,325 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:49,337 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:49,342 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:49,343 : INFO : EPOCH - 67 : training on 59890 raw words (36130 effective words) took 0.6s, 59585 effective words/s\n",
            "2021-01-19 17:46:49,945 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:49,946 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:49,962 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:49,963 : INFO : EPOCH - 68 : training on 59890 raw words (36028 effective words) took 0.6s, 58354 effective words/s\n",
            "2021-01-19 17:46:50,552 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:50,554 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:50,567 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:50,568 : INFO : EPOCH - 69 : training on 59890 raw words (35944 effective words) took 0.6s, 60030 effective words/s\n",
            "2021-01-19 17:46:51,166 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:51,167 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:51,182 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:51,182 : INFO : EPOCH - 70 : training on 59890 raw words (36002 effective words) took 0.6s, 58781 effective words/s\n",
            "2021-01-19 17:46:51,780 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:51,782 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:51,796 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:51,797 : INFO : EPOCH - 71 : training on 59890 raw words (35930 effective words) took 0.6s, 59143 effective words/s\n",
            "2021-01-19 17:46:52,393 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:52,400 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:52,408 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:52,409 : INFO : EPOCH - 72 : training on 59890 raw words (35888 effective words) took 0.6s, 59130 effective words/s\n",
            "2021-01-19 17:46:53,012 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:53,013 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:53,027 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:53,028 : INFO : EPOCH - 73 : training on 59890 raw words (36057 effective words) took 0.6s, 58522 effective words/s\n",
            "2021-01-19 17:46:53,615 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:53,616 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:53,630 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:53,631 : INFO : EPOCH - 74 : training on 59890 raw words (36004 effective words) took 0.6s, 59946 effective words/s\n",
            "2021-01-19 17:46:54,237 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:54,243 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:54,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:54,255 : INFO : EPOCH - 75 : training on 59890 raw words (36067 effective words) took 0.6s, 58200 effective words/s\n",
            "2021-01-19 17:46:54,849 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:54,852 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:54,865 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:54,865 : INFO : EPOCH - 76 : training on 59890 raw words (35992 effective words) took 0.6s, 59157 effective words/s\n",
            "2021-01-19 17:46:55,459 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:55,464 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:55,475 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:55,476 : INFO : EPOCH - 77 : training on 59890 raw words (36019 effective words) took 0.6s, 59364 effective words/s\n",
            "2021-01-19 17:46:56,073 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:56,074 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:56,089 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:56,090 : INFO : EPOCH - 78 : training on 59890 raw words (35931 effective words) took 0.6s, 59003 effective words/s\n",
            "2021-01-19 17:46:56,685 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:56,690 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:56,702 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:56,703 : INFO : EPOCH - 79 : training on 59890 raw words (35921 effective words) took 0.6s, 59257 effective words/s\n",
            "2021-01-19 17:46:57,299 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:57,300 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:57,315 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:57,316 : INFO : EPOCH - 80 : training on 59890 raw words (35949 effective words) took 0.6s, 59293 effective words/s\n",
            "2021-01-19 17:46:57,914 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:57,916 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:57,929 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:57,930 : INFO : EPOCH - 81 : training on 59890 raw words (35880 effective words) took 0.6s, 59199 effective words/s\n",
            "2021-01-19 17:46:58,548 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:58,551 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:58,566 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:58,567 : INFO : EPOCH - 82 : training on 59890 raw words (35890 effective words) took 0.6s, 56723 effective words/s\n",
            "2021-01-19 17:46:59,158 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:59,159 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:59,174 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:59,174 : INFO : EPOCH - 83 : training on 59890 raw words (35978 effective words) took 0.6s, 59457 effective words/s\n",
            "2021-01-19 17:46:59,771 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:46:59,773 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:46:59,788 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:46:59,789 : INFO : EPOCH - 84 : training on 59890 raw words (35890 effective words) took 0.6s, 58691 effective words/s\n",
            "2021-01-19 17:47:00,384 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:00,389 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:00,400 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:00,401 : INFO : EPOCH - 85 : training on 59890 raw words (35962 effective words) took 0.6s, 59350 effective words/s\n",
            "2021-01-19 17:47:00,986 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:00,992 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:01,001 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:01,001 : INFO : EPOCH - 86 : training on 59890 raw words (35937 effective words) took 0.6s, 60162 effective words/s\n",
            "2021-01-19 17:47:01,601 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:01,606 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:01,615 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:01,616 : INFO : EPOCH - 87 : training on 59890 raw words (36047 effective words) took 0.6s, 58902 effective words/s\n",
            "2021-01-19 17:47:02,211 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:02,212 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:02,229 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:02,230 : INFO : EPOCH - 88 : training on 59890 raw words (35984 effective words) took 0.6s, 58853 effective words/s\n",
            "2021-01-19 17:47:02,821 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:02,822 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:02,837 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:02,837 : INFO : EPOCH - 89 : training on 59890 raw words (35987 effective words) took 0.6s, 59549 effective words/s\n",
            "2021-01-19 17:47:03,437 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:03,439 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:03,453 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:03,454 : INFO : EPOCH - 90 : training on 59890 raw words (35930 effective words) took 0.6s, 58573 effective words/s\n",
            "2021-01-19 17:47:04,042 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:04,050 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:04,058 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:04,059 : INFO : EPOCH - 91 : training on 59890 raw words (35958 effective words) took 0.6s, 59671 effective words/s\n",
            "2021-01-19 17:47:04,652 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:04,657 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:04,668 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:04,668 : INFO : EPOCH - 92 : training on 59890 raw words (35873 effective words) took 0.6s, 59275 effective words/s\n",
            "2021-01-19 17:47:05,276 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:05,279 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:05,292 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:05,292 : INFO : EPOCH - 93 : training on 59890 raw words (36016 effective words) took 0.6s, 57979 effective words/s\n",
            "2021-01-19 17:47:05,877 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:05,887 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:05,894 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:05,895 : INFO : EPOCH - 94 : training on 59890 raw words (35975 effective words) took 0.6s, 60086 effective words/s\n",
            "2021-01-19 17:47:06,486 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:06,495 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:06,503 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:06,504 : INFO : EPOCH - 95 : training on 59890 raw words (36089 effective words) took 0.6s, 59427 effective words/s\n",
            "2021-01-19 17:47:07,098 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:07,099 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:07,117 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:07,118 : INFO : EPOCH - 96 : training on 59890 raw words (36030 effective words) took 0.6s, 59618 effective words/s\n",
            "2021-01-19 17:47:07,718 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:07,720 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:07,735 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:07,736 : INFO : EPOCH - 97 : training on 59890 raw words (35927 effective words) took 0.6s, 59466 effective words/s\n",
            "2021-01-19 17:47:08,330 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:08,331 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:08,344 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:08,345 : INFO : EPOCH - 98 : training on 59890 raw words (36016 effective words) took 0.6s, 59528 effective words/s\n",
            "2021-01-19 17:47:08,965 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:08,967 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:08,988 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:08,989 : INFO : EPOCH - 99 : training on 59890 raw words (36106 effective words) took 0.6s, 56304 effective words/s\n",
            "2021-01-19 17:47:09,583 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-01-19 17:47:09,587 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-01-19 17:47:09,599 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-01-19 17:47:09,600 : INFO : EPOCH - 100 : training on 59890 raw words (35956 effective words) took 0.6s, 59127 effective words/s\n",
            "2021-01-19 17:47:09,600 : INFO : training on a 5989000 raw words (3597478 effective words) took 61.3s, 58687 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=1744, size=200, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whigosvqSvto"
      },
      "source": [
        "**Zad. 6: Odkomentuj poniższe linie i zobacz jak można wykorzystać uzyskany model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqv705bDSvtp",
        "outputId": "291cb69d-4b3a-421d-ed1f-dff8b40ce9fe"
      },
      "source": [
        "model.wv.most_similar(positive=['human', 'crime'], negative=['party'], topn=1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-19 17:48:15,713 : INFO : precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rights', 0.46687859296798706)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "C-nBIn4USvtp",
        "outputId": "fe4e31d6-412e-417d-e692-b28ae082a780"
      },
      "source": [
        "model.wv.doesnt_match(\"input is lunch he sentence cat\".split())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-19 17:48:30,111 : WARNING : vectors for words {'cat', 'lunch', 'input'} are not present in the model, ignoring these words\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'is'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHrQmm5wSvtq",
        "outputId": "fe0978ea-dd74-4340-a738-d9f838d9b8c8"
      },
      "source": [
        "print(model.wv.similarity('human', 'tree'))\n",
        "print(model.wv.similarity('crime', 'murder'))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.01882398\n",
            "0.2666616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyIIMPXkSvtq"
      },
      "source": [
        "**Uwagi dodatkowe:**\n",
        "+ uczenie modelu można zrównoleglić, ale trzeba doinstalować [Cythona](http://cython.org/)\n",
        "+ wytrenowany model można łatwo zapisać do pliku za pomocą: `model.save(path)`\n",
        "+ równie łatwo można go później wczytać: `model = gensim.models.Word2Vec.load(path)`\n",
        "+ ponieważ uczenie jest przyrostowe, można łatwo rozszerzyć istniejący słownik i douczyć model na nowych zdaniach:\n",
        "```\n",
        "model = gensim.models.Word2Vec.load(path)\n",
        "more_sentences = [['Advanced', 'users', 'can', 'load', 'a', 'model', 'and', 'continue', \n",
        "                  'training', 'it', 'with', 'more', 'sentences']]\n",
        "model.build_vocab(more_sentences, update=True)\n",
        "model.train(more_sentences, )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOwbWoG5Svtq"
      },
      "source": [
        "## Wykorzystanie gotowego modelu do klasyfikacji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87KlzyxZSvtr"
      },
      "source": [
        "Póki co sami trenowaliśmy word2vec i to na niedużych zbiorach danych. Na szczęście są już gotowe modele (przynajmniej dla języka angielskiego) nauczone na miliardach dokumentów i zawierające miliony słów. Przydatna lista takich modeli (wraz z kodem tworzącym usługę sieciową wykorzystującą model...) pod adresem: https://github.com/3Top/word2vec-api.\n",
        "\n",
        "**Zad. 7: Pobierz korpus Google News i zapisz pobrany plik do folderu data. Następnie wykonaj poniższy kod. Ta operacja zajmie jakieś 3-4 minuty i zużyje ok. 4 GB RAMU.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyYXMse_xNXj",
        "outputId": "cac2e636-e0bb-45aa-c639-ea9871b1638c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdux0Iu6Svtr",
        "outputId": "6fcb7530-765b-46b9-8ade-79d370477e94"
      },
      "source": [
        "%%time \n",
        "wv = gensim.models.KeyedVectors.load_word2vec_format(\"drive/MyDrive/Word2VecNotebook/data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "wv.init_sims(replace=True)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-19 18:20:59,808 : INFO : loading projection weights from drive/MyDrive/Word2VecNotebook/data/GoogleNews-vectors-negative300.bin.gz\n",
            "2021-01-19 18:22:52,904 : INFO : loaded (3000000, 300) matrix from drive/MyDrive/Word2VecNotebook/data/GoogleNews-vectors-negative300.bin.gz\n",
            "2021-01-19 18:22:52,910 : INFO : precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 10s, sys: 3.59 s, total: 2min 13s\n",
            "Wall time: 2min 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paGM5UvSSvts"
      },
      "source": [
        "**Zad. 8: Zobacz jak działa model nauczony na tak dużym korpusie.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6rrebZaSvtt",
        "outputId": "a14f70b2-4753-4bfc-c897-7846f6c94b6b"
      },
      "source": [
        "wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7118192911148071)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "p1GLgA_nSvtu",
        "outputId": "bb992972-4ff9-4c4e-f4f9-dabad32c8583"
      },
      "source": [
        "wv.doesnt_match(\"breakfast cereal dinner lunch\".split())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cereal'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHgtaWjCSvtv",
        "outputId": "5dfce289-468a-4b75-c5c8-504c131c78c9"
      },
      "source": [
        "print(wv.similarity('woman', 'man'))\n",
        "print(wv.similarity('woman', 'cat'))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.76640123\n",
            "0.32413527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzCX9nf-Svtv"
      },
      "source": [
        "Ok, super. Mam świetny model, mogę nim podpowiadać słowa, wynajdować niepasujące elementy, uzupełniać zdania, znajdować synonimy, itd. Ale czy da się to jakoś wykorzystać do klasyfikacji? word2vec ma wektor na każde słowo - jak z tego zrobić wektor na ciąg słów?\n",
        "\n",
        "**Odpowiedź: można uśrednić znaczenie słów w dokuemncie poprzez zsumowanie wektorów wszystkich słów.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WtyF7sW_Svtv"
      },
      "source": [
        "def word_averaging(wv, words):\n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    for word in words:\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        elif word in wv.vocab:\n",
        "            mean.append(wv.vectors_norm[wv.vocab[word].index])\n",
        "            all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    if not mean:\n",
        "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
        "        return np.zeros(wv.layer_size,)\n",
        "\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "    return np.vstack([word_averaging(wv, review) for review in text_list ])\n",
        "\n",
        "def w2v_tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SECk36OHSvtw"
      },
      "source": [
        "Bardzo szybko spróbujemy zastosować to podejście do predykcji gatunku filmu na podstawie jego opisu. Poniżej kod wczytujący ciekawy zbiór danych oraz pokazujący jakie ma klasy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "ZtGx9xubSvtx",
        "outputId": "3f38d485-d8b3-4711-831a-70a3db1bf4bf"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "df = pd.read_csv('data/tagged_plots_movielens.csv')\n",
        "df = df.dropna()\n",
        "\n",
        "print(df.head())\n",
        "df.tag.value_counts().plot(kind=\"bar\", rot=0)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-19 18:24:41,594 : INFO : NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  ...        tag\n",
            "0           0  ...  animation\n",
            "1           1  ...    fantasy\n",
            "2           2  ...     comedy\n",
            "3           3  ...     action\n",
            "4           4  ...    romance\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fbf3623ef28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dkvao8wSvty"
      },
      "source": [
        "Szybko dzielimy dane na zbiór uczący i testowy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R8559WLSvty"
      },
      "source": [
        "train_data, test_data = train_test_split(df, test_size=0.1, random_state=42)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaY2UeVBSvty"
      },
      "source": [
        "Tokenizujemy dane i wyliczamy reprezentację wektorową (za pomocą zsumowanych wektorów słów wrod2vec):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl8JIRiXSvty"
      },
      "source": [
        "test_tokenized = test_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values\n",
        "train_tokenized = train_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values\n",
        "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
        "X_test_word_average = word_averaging_list(wv,test_tokenized)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-5IWIBjSvty"
      },
      "source": [
        "Uczymy i testujemy klasyfikator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npdd2wD1Svtz",
        "outputId": "5203bee7-b37c-4139-8952-833f5d31524d"
      },
      "source": [
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "\n",
        "logreg = logreg.fit(X_train_word_average, train_data['tag'])\n",
        "predicted = logreg.predict(X_test_word_average)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98S6nfxZSvtz"
      },
      "source": [
        "Patrzymy jak nam poszło:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b9W97-mSvtz",
        "outputId": "2d7059e7-1c39-4e18-bb5c-dcb7644d1fe6"
      },
      "source": [
        "print('Trafność klasyfikacji %s' % accuracy_score(test_data.tag, predicted))\n",
        "cm = confusion_matrix(test_data.tag, predicted)\n",
        "print('Macierz pomyłek\\n %s' % cm)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trafność klasyfikacji 0.5349794238683128\n",
            "Macierz pomyłek\n",
            " [[22  2 10  0  2  6]\n",
            " [ 3  7 10  5  3  3]\n",
            " [ 3  4 60  3 12  4]\n",
            " [ 2  3  5  5  0  1]\n",
            " [ 3  0 13  1 16  2]\n",
            " [ 6  0  4  1  2 20]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em9Fhod0Svtz"
      },
      "source": [
        "Jak na brak porządnego przetwarzania wstępnego, nie jest to zły wynik. Mam nadzieję, że ten przykład pokazał jak można wykorzystać word2vec do tworzenia atrybutów dla problemów klasyfikacyjnych."
      ]
    }
  ]
}