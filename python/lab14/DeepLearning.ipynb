{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Uczenie głębokie cz. I\n",
    "\n",
    "Ten notatnik ma na celu przedstawienie jednego ze sposobów tworzenia głębokich sieci neuronowych w Pythonie. W trakcie zadania najpierw stworzymy prostą sieć konwolucyjną, a następnie zbadamy jej proces uczenia się. Na tych zajęciach wykorzystamy biblioteki [Keras](https://keras.io/) i [Tensorflow](https://www.tensorflow.org/).\n",
    "\n",
    "Po wykonaniu tego zadania powinieneś:\n",
    "+ wiedzieć z jakich części składają się sieci konwolucyjne,\n",
    "+ potrafić uruchomić sieć neuronową na własnych danych,\n",
    "+ wiedzieć jak wczytać i wykorzystać gotowy model,\n",
    "+ zwizualizować i śledzić na żywo proces uczenia się sieci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prosta sieć konwolucyjna\n",
    "\n",
    "Najpierw załadujemy odpowiednie biblioteki i wczytamy mały zbiór obrazów do uczenia się rozpoznawania liczb. \n",
    "\n",
    "**Zad. 1:Przejrzyj komentarze przy operacjach `import`, aby zapoznać się z nazewnictwem w bibliotece Keras.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(23)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Typ sieci\n",
    "from keras.models import Sequential # tworzymy typową sieć jednokierunkową (bez rekurencji)\n",
    "\n",
    "# Typy warstw\n",
    "from keras.layers import Dense # warstwa z w pełni połączona (wszystkie neurony z poprzedniej warstwy do każdego w kolejnej)\n",
    "from keras.layers import Dropout # mechanizm losowego wyłączania neuronów (pozwala uniknąć przeuczenia)\n",
    "from keras.layers import Activation # aktywacja, czyli funkcja przyjmująca jedną liczbę (np. sigmoid, softmax) \n",
    "from keras.layers import Flatten # spłaszczenie wejścia (np. zamiana tensora o wymiarach 64x32x32 na wektor o długości 65536)\n",
    "from keras.layers import Conv2D # warstwa konwolucyjna, tworzy filtry o zadanym rozmiarze w zadanych odstępach\n",
    "from keras.layers import MaxPooling2D # warstwa zmniejszająca obraz poprzez operacje max na zadanym rozmiarze pikseli\n",
    "\n",
    "# Optymalizator i funkcja straty\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "\n",
    "# \"Przydasie\"\n",
    "from keras import utils as k_utils\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import TensorBoard\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas na zbiór danych. Skorzystamy ze standardowego zbioru treningowego do omawiania sieci konwolucyjnych, czyli [MNIST](http://yann.lecun.com/exdb/mnist/). Zbiór składa się z małych obrazków, więc będziemy w stanie nauczyć sieć na zwykłym komputerze, a sieci konwolucyjne są tu jak najbardziej potrzebne do uzyskania sensownych wyników.\n",
    "\n",
    "Biblioteka Keras ma funkcję pobierającą ten zbiór z sieci, z czego skrzętnie skorzystamy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 2: Sprawdź jakie wymiary mają załadowane obrazki. Przypisz te wartości do zmiennych img_rows, img_cols. Sprawdź również liczbę klas i przypisz tę wartość to zmiennej `num_classes`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28 10\n"
     ]
    }
   ],
   "source": [
    "# sprawdź wymiary zbioru danych\n",
    "\n",
    "img_rows = X_train[0].shape[0]\n",
    "img_cols = X_train[0].shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(img_rows, img_cols, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 3: Wyświetl pierwszy rysunek ze zbior.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0f88ab9160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstępne przetwarzanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponieważ obrazy mogą mieć różną liczbę kolorów (kanałów), musimy powiedzieć Kerasowi jaki format danych będzie przetwarzać. Obrazy mogą mieć jeden kanał (skala szarości), trzy kanały (RGB), a czasami więcej niż trzy kanały (np. zdjęcia satelitarne). Dane w zbiorze MNIST są zakodowane w skali szarości, a zatem mają jeden kanał.\n",
    "\n",
    "**Zad. 4: Zmień wymiarowość danych wejściowych zgodnie z poniższym wzorem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_wierszy = img_rows\n",
    "l_kolumn = img_cols\n",
    "l_kanalow = 1\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], l_wierszy, l_kolumn, l_kanalow)\n",
    "X_test = X_test.reshape(X_test.shape[0], l_wierszy, l_kolumn, l_kanalow)\n",
    "input_shape = (l_wierszy, l_kolumn, l_kanalow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oprócz określenia wymiaru danych wejściowych musimy upewnić się, że są one typu `float32`. Ponadto warto ustandaryzować dane wejściowe lub sprowadzić je do zakresu [0-1]. My uczynimy to drugie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ostatnia rzecz, którą musimy zrobić to zakodować klasy (`y`) w sposób binarny. Czyli zamiast klas `1, 2, ... 10` itd., chcemy uzyskać kodowanie w postaci `[1 0 0 0 0 0 0 0 0 0], [0 1 0 0 0 0 0 0 0 0] ... [0 0 0 0 0 0 0 0 0 1]`. W Kerasie posłuży nam do tego funkcja `to_categorical()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = k_utils.to_categorical(y_train, num_classes)\n",
    "y_test = k_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tworzenie sieci\n",
    "\n",
    "W Kerasie **architekturę sieci** definiuje się poprzez podanie typu sieci i kolejnych warstw. Dla każdej warstwy określa się typ neuronów, ich liczbę i (jeśli to konieczne) ich parametry.\n",
    "\n",
    "Poniżej zdefiujemy prostą sieć konwolucyjną. Będzie się ona składała tylko z dwóch warstw konwolucyjnych, bo zdjęcia na których operujemy są bardzo małe. Większe zdjęcia wymagałyby więcej warstw. Uwaga! W uporszczeniu: im większa sieć tym większe wymagania pamięciowe i czasowe!\n",
    "\n",
    "Nasza architektura to (porównaj z poniższym kodem):\n",
    "- **warstwa konwolucyjna** tworząca **32 filtry** o rozmiarze **3x3**. Każdy filtr stanowi wejście dla funkcji atywacyjnej typu **ReLU**. Zauważ, że w pierwszej warstwie sieci musimy podać wymiary (warstwę wejściową). W kolejnych warstwach nie jest to konieczne, bo warstwy wiedzą jak mają się łączyć.\n",
    "- **warstwa konwolucyjna** tworząca **64 filtry** o rozmiarze **3x3** z **ReLU**. \n",
    "- **warstwa pooling** zmiejszająca obraz o połowę\n",
    "- **warstwa dropout** losowo wyłączają 1/4 neuronów przy każdym przejściu\n",
    "- **warstwę spłaszczającą** wielowymiarowe konwolucje w wektor\n",
    "- **w pełni połączona warstwa** 128 neuronów\n",
    "- **kolejny dropout**\n",
    "- **warstwa wyjściowa** posiadającą tyle neuronów ile mamy klas\n",
    "\n",
    "**Zad. 5: W ostatniej warstwie nie podałem funkcji aktywacji. Wróć do slajdów i sprawdź jaka funkcja aktywacji nada się do klasyfikacji.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po stworzeniu sieci w Kerasie trzeba ją skompilować. W tym kroku Keras dostosowuje kod pod wykonanie na wybranym backendzie. W naszym przypadku będzie to Tensorflow, ale równie dobrze mogłyby to być platformy Theano lub CNTK.\n",
    "\n",
    "Podczas kompilacji podajemy **funkcję straty**, **optymalizator**. Ponadto można podać szereg miar do śledzenia (typowo przy klasyfikacji accuracy).\n",
    "\n",
    "**Zad. 6: Wróć do slajdów i sprawdź jaka funkcja straty nada się do klasyfikacji wieloklasowej. Jako optymalizator wybierz Adadelta, dzięki temu nie będziesz musiał podawać learning rate. Protip: użyj Tab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.CategoricalCrossentropy(),\n",
    "              optimizer=optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy sieć. Teraz nic tylko odpalić uczenie. Robiąc to musimy podać **batch_size** (czyli ile przykładów przepuszczamy przez sieć między aktualizacją wag) i liczbę **epok** (czyli ile razy chcemy przepuścić przez sieć cały zbiór danych). Zbiór walidacyjny będzie nam mówił na ile dobrze sieć działa na danych, których wcześniej nie widzieliśmy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      " 47/469 [==>...........................] - ETA: 1:08 - loss: 2.2993 - accuracy: 0.1086"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e117d42e6234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           validation_data=(X_test, y_test))\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=12,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "# Jak kod ruszy, oszacuj ile zajmie uczenie sieci (ETA jednej epoki x liczba epok).\n",
    "# Jeśli wychodzi Ci więcej niż 3 minuty zatrzymaj uczenie i czytaj dalej..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spójrzmy na ETA. Aha, ale to tylko 1 epoka z 12... Hmmm.... Trochę będzie trzeba poczekać. Zatrzymaj powyższy blok kodu. Zaraz coś z tym zrobimy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU\n",
    "\n",
    "Uczenie sieci to bardzo czasochłonny proces. Na szczeście, ze względu na swoją modularność, sieci bardzo łatwo się zrównolegla. Stąd gotowość większości bibliotek do współpracy z kartami graficznymi (jednakże obecnie są to praktycznie tylko karty NVidii).\n",
    "\n",
    "Ponieważ prawdopodobnie nie masz komputera z super-ekstra kartą graficzną do obliczeń równoległych, skorzystamy z karty graficznej w chmurze. AWS, Google Cloud, Microsoft Azure chętnie udostępnią swoje moce obliczeniowe za drobną opłatą. Maszyny wirtualne z GPU są stosunkowo drogie w porównaniu do innych maszyn, ale przy naliczaniu godzinowym (a niekiedy sekundowym) jest to bardzo interesująca opcja. Jeśli masz dostęp do jednej z tych platform, albo lepiej dostęp i niewykorzystany kredyt, warto z tego skorzystać.\n",
    "\n",
    "Aby zaoszczędzić sobie czasu nie będziemy stawiać pełnoprawnych wirtualek na żadnej z wymienionych platform. Zamiast tego szybko założymy konto na stronie Crestle, gdzie będziemy mieli godzinę darmowych obliczeń (z naliczaniem sekundowym):\n",
    "1. Wejdź na https://www.crestle.ai/ i załóż konto.\n",
    "2. Po zalogowaniu upewnij się, że włączony jest przełącznik **Enable GPU**\n",
    "3. Kliknij **Start Jupyter**\n",
    "4. Po zalogowaniu po prawej stronie nad listą plików kliknij na **Upload** (a potem znów upload)\n",
    "5. Wgraj notebook\n",
    "6. Odpal kod\n",
    "\n",
    "**Zad 7. Wykonaj powyższe kroki i naucz sieć z tego notebooka na GPU. Uwaga Crestle będzie się trochę rozgrzewał zanim ruszy pełną parą. Po zakończeniu obliczeń zapisz model do pliku za pomocą poniższego kodu. Następnie ściągnij utworzony plik modelu z Crestle i zamknij zdalnego Jupytera.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"gpu_cnn_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytywanie modelu i predykcje\n",
    "\n",
    "Uczenie się trwa długo, ale jak już mamy nauczoną sieć to wcale nie zajmuje ona aż tyle miejsca na dysku i działa bardzo szybko. Sprawdźmy czy tak rzeczywiście jest.\n",
    "\n",
    "**Zad. 8: Wczytaj model nauczony na GPU na swoim lokalnym komputerze i dokonaj predykcji zgodnie z poniższym szablonem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 309 ms, sys: 12.2 ms, total: 321 ms\n",
      "Wall time: 306 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gpu_model = load_model(\"gpu_cnn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 przykładów\n",
      "loss=0.3814, accuracy=0.8962\n",
      "CPU times: user 7.74 s, sys: 111 ms, total: 7.86 s\n",
      "Wall time: 2.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score = gpu_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('{0} przykładów'.format(X_test.shape[0]))\n",
    "print('loss={0:.4f}, accuracy={1:.4f}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U mnie na laptopie wyszło ok. 9 sekund. Dzieląc to przez liczbę przykładów wychodzi 10000/9 = 1111 obrazów na sekundę, czyli nie tak źle. Zwłaszcza, że mój komputer ma ponad 5 lat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow udostępnia świetne narzędzie do wizualizacji procesu uczenia sieci o nazwie **Tensorboard**. Aby je uruchomić odpal Anaconda Prompt i wpisz następujące polecenie:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=\"pelna_sciezka_do_tego_notebooka/logs\"\n",
    "```\n",
    "\n",
    "**Zad. 9: Odpal Tensorboarda a następnie uruchom (lokalnie) poniższy kod. W trakcie gdy sieć się uczy wejdź na stronę Tensorboard i sledź zmiany.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "Epoch 1/12\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 2.3080 - accuracy: 0.1198 - val_loss: 2.3055 - val_accuracy: 0.1430\n",
      "Epoch 2/12\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 2.2956 - accuracy: 0.1289 - val_loss: 2.3038 - val_accuracy: 0.1490\n",
      "Epoch 3/12\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 2.2959 - accuracy: 0.1145 - val_loss: 2.3022 - val_accuracy: 0.1510\n",
      "Epoch 4/12\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 2.2997 - accuracy: 0.1176 - val_loss: 2.3006 - val_accuracy: 0.1560\n",
      "Epoch 5/12\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 2.2940 - accuracy: 0.1256 - val_loss: 2.2990 - val_accuracy: 0.1590\n",
      "Epoch 6/12\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 2.2961 - accuracy: 0.1634 - val_loss: 2.2975 - val_accuracy: 0.1590\n",
      "Epoch 7/12\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 2.2976 - accuracy: 0.1357 - val_loss: 2.2959 - val_accuracy: 0.1640\n",
      "Epoch 8/12\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 2.2966 - accuracy: 0.1064 - val_loss: 2.2942 - val_accuracy: 0.1710\n",
      "Epoch 9/12\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 2.2932 - accuracy: 0.1308 - val_loss: 2.2925 - val_accuracy: 0.1760\n",
      "Epoch 10/12\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 2.2919 - accuracy: 0.1044 - val_loss: 2.2910 - val_accuracy: 0.1860\n",
      "Epoch 11/12\n",
      "16/16 [==============================] - 1s 95ms/step - loss: 2.2888 - accuracy: 0.1227 - val_loss: 2.2893 - val_accuracy: 0.1870\n",
      "Epoch 12/12\n",
      "16/16 [==============================] - 1s 95ms/step - loss: 2.2865 - accuracy: 0.1529 - val_loss: 2.2877 - val_accuracy: 0.1940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0f7423a6d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 12\n",
    "batch_size = 64\n",
    "dataset_size = 1000\n",
    "\n",
    "# Protip: w nazwie modelu zawrzyj wartości parametrów, wtedy będzie można łatwo porównywać różne architektury/parametry\n",
    "model_name = \"cpu_cnn_model\" + \\\n",
    "    \"_e=\" + str(epochs) + \"_b=\" + str(batch_size) + \\\n",
    "    \"_n=\" + str(dataset_size) + \"_t=\" + time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "\n",
    "tb_callback = TensorBoard(log_dir=\"./logs/\" + model_name, batch_size=128, histogram_freq=-1,\n",
    "                          write_graph=True, write_images=False)\n",
    "\n",
    "model.fit(X_train[:dataset_size,:,:,:], y_train[:dataset_size,:],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test[:dataset_size,:,:,:], y_test[:dataset_size,:]),\n",
    "          callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 10: Zmień batch size na 64, zbuduj i skompiluj ponownie sieć (kod z zad. 5 i 6) i odpal uczenie jeszcze raz. Porównaj krzywe uczenia w Tensorboard.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poznałeś podstawy tego co jest potrzebne aby nauczyć własną głęboką sieć neuronową od zera. O tym jakie inne architektury zaproponowano do różnych zadań i jak przyspieszyć proces uczenia sieci dla wielu standardowych problemów dowiesz się na następnych zajęciach."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
